// Paths //

// environment variables
root_dir = ${DISCRETE_PROJECT_DIR}
data_dir = ${DISCRETE_DATA_DIR}
// experiment args (should change)
expname = exp
runname = demo
sub_runname = none
// set file logic (do not change)
exp_dir = ${root_dir}"/"${expname}
run_dir = ${exp_dir}"/"${runname}
sub_run_dir = ${run_dir}"/"${sub_runname}

// Gerneral //

seed = 1234
ckpt_path = none // current
task = ag // ag, yelp-full, db
phase = pretrain // pretrain, target_train, analyze

// vocab size
max_word_v_size = 30000
word_freq_thresh = 0

model = quantized_transformer // quantized_transformer, lstm, cbow, transformer
d_model = 64


analyze =
{
  batch_size = 64
  split = [train, val, test] // any combination of train, val, test
                             // e.g. train, or [train], or [train, val]
}


// Base Encoders //

transformer =
{
  d_model = ${d_model}
  d_ffn = 256
  enc_nlayer = 1
  nhead = 4
  dropout = 0.2
}

lstm =
{
  d_model = ${d_model}
  h_dim = 50
  enc_nlayers = 1
  enc_ndirections = 2
}

cbow =
{
  d_model = ${d_model}
}


// Training //

// pretrain
pretrain =
{
  grad_norm = 5
  batch_size = 64
  max_epochs = 5
  log_every = 100
  val_data_limit = 5000
  val_interval = 500
  patience = 5 // early stopping based on val perplexity

  // optimizer
  use_noam = 0 // 0, 1, whether use noam optimizer
  // i.e. adam with inverse square root schedule
  lr = 1e-3 // use adam with constant lr, when not use noam

  em_train = 0 // train encoder and decoder alternatingly
  em_iter = 1 // >= 1, num of of E step

  tensorboard = 1 // log train and val progress to tensorboard

}

// target train
target-tmpl =
{
  grad_norm = 5
  batch_size = 64
  val_data_limit = 5000
  log_every = 100
  val_interval = 500
  lr = 3e-4
  patience = 5
  max_epochs = 10
  use_noam = 0 // 0, 1

  # TODO: change name
  train_ratio = none // label percentage (0-1]
  train_num = none // label number
  sample_first = 1
  test = 0
}

target-200-tmpl
{
  train_num = 200
  batch_size = 8
  patience = 10
  val_interval = 25
  max_epochs = 1000
}

target-500-tmpl
{
  train_num = 500
  batch_size = 16
  patience = 10
  val_interval = 30
  max_epochs = 1000
}

target-2500-tmpl
{
  train_num = 2500
  batch_size = 32
  patience = 5
  val_interval = 100
  max_epochs = 1000
}


target-full-tmpl
{
  train_num = none
  batch_size = 64
  patience = 5
  val_interval = 500
  max_epochs = 10
}

target=${target-tmpl}

// noam optimizer
noam =
{
  warmup = 2000 // important
  factor = 1
  lr = 0
  beta1 = 0.9
  beta2 = 0.98
  eps = 1e-9
  weight_decay = 0
}


// Discretization Layer //
quantizer =
{
  M = 1 // split
  K = 10  // embedding numbers
  level = sentence // sentence, word
  type = vq // vq, concrete, em
}


// Vector Quantization
vq =
{
  commitment_cost = 0.25
  init = xavier_uniform // initialize codebook vector
  use_ema = 0 // exponential moving average
  ema =
  {
    decay = 0.99
    epsilon = 1e-9
  }
}


// Concrete Relaxation
concrete =
{
  // ST Gumbel
  hard = 1

  // posterior temperature: ST - backward pass, soft: forward and backward
  tau =
  {
    mode = fix // 'fix', 'anneal', 'train'
    init = 1

    anneal =
    {
      base = 0.5 //
      rate = 1e-4 //  1e-4, 1e-5, 3e-5
      interval= 1000 // 500, 1000, 2000
    }

  }

  // for KL
  kl =
  {
    type = categorical // 'relaxed', 'categorical'
    prior_logits = uniform // 'train', 'uniform'
    prior_tau = 1 // 'train', fixed positive float, for relaxed kl only

    beta = 1 // 0 - autoencoder, 1 - vae

    fbp_threshold = 0 // 1, 3, 5, 7, 10, 1e7 - autoencoder, 0 - no fbp, vae
    fbp_ratio = none // between 0 - 1

  }
}

// Hard Expectation Maximization
em =
{
  relax = 1 // 1 or 0
}


// Classifier //
classifier =
{
  aggregate = mean // mean, max, attn, final_state (lstm), none (no pooling)
  freeze_encoder = 1 // 0 - unfreeze, 1 - freeze all
  type = log_reg

  add_layer = none // lstm, transformer, none
  // ffn (only for quantizer.level = sentence, aggregate = none)??

  reembedding = 0 // 0 - use codebook vector, 1 - learn new embedding

  merge_word = concat // concat, sum, sum_tanh, sum_relu
  // only for quantizer.level = word, aggregate inter-word idx, should always use concat when M = 1
}

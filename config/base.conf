include "default.conf"

expname = demo_exp
runname = demo_run
sub_runname = none
phase = pretrain
ckpt_path = none

model = quantized_transformer
task = ag // ag, db, yelp-full
seed = 1234


quantizer
{
  M=4
  K=256
  type = vq // vq, concrete, em
  level = sentence // sentence, word
}


transformer
{
  nhead = 4 // 2, 4, 8, 16
  dropout = 0.2 // 0.1, 0.2
}

vq
{
  use_ema = 0 // 0, 1
  commitment_cost = 1e-3 // 1e-1, 1e-2, 1e-3
}

concrete
{
  tau.init = 0.67 // 0.33, 0.4, 0.5, 0.67, 1
  kl.fbp_ratio = 0.6 // 0.2, 0.4, 0.6, 0.8, 10

  kl.type = categorical
  hard = 1
}

em
{
  relax = 1
}

noam
{
  warmup = 2000
}

pretrain
{
  max_epochs = 10
  patience = 5
  lr = 1e-3

  batch_size = 64 // max bsz allowed by 12g gpu, ag: 64, db/yelp-full: 32
  use_noam = 0 // 0, 1
  em_train = 0 // 0, 1
  em_iter = 1 // 1, 3
}


classifier
{
  add_layer = none // transformer, none
  reembedding = 0 // 0 - use codebook vector, 1 - learn new embedding

  aggregate = mean
  freeze_encoder = 1
  type = log_reg
}


target=${target-tmpl}${target-200-tmpl}{test=0}

analyze
{
  split = [train, val, test]
  batch_size = 64
}
